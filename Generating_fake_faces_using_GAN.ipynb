{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ingasha-Sharon/GENERATING-FAKE-FACES-USING-GAN/blob/main/Generating_fake_faces_using_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJaYMzam3SKm",
        "outputId": "b25dbceb-e44e-4215-cf63-cfa50bd19b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading face-mask-lite-dataset, 25002249935 bytes compressed\n",
            "[==================================================] 25002249935 bytes downloaded\n",
            "Downloaded and uncompressed: face-mask-lite-dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'face-mask-lite-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F770718%2F1328233%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240708%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240708T194053Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D018339d9efda46921541189f1750654747d136da594723e6f0f3338aa150dd69b52e7cccd7104660034eae974ff197764afd3b0dbb78cf160d53cca2029a6ce9d29f686675a78f462f39d7db56dc9391977f6c844d1139489835243654d7c8367b98d659131d55bc2ef6f31201fbe497728e734191fd6bfce98e18314b967e85cec235b31bfae998d80ddc8df973d003cd1f319432b6a213339e27ae25422960f1f17bf432ecc901239f70e0072cd876e9ecea850080a6e45e61f946fcb69fd199986804256b01d93305c11300a1cb6827e1d0bf730024680a3b436c9bb691bd472bbf51ba03ab28728f377fa58c41618d3588263918092481231e095c264526'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PcdjT8l3SKo"
      },
      "source": [
        "# Introduction\n",
        "<img src = 'https://cdn-images-1.medium.com/max/900/1*TKr1dtcNgJCA8uYY1OhmSg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq2soOCd3SKq"
      },
      "source": [
        "Generative Adversarial Networks (GANs) are generative models. They are uses unsupervised technique to generate new things. GAN models learns pattern in input data in such a way that they can generate new sample which resemble with the input data. The main aim of generative adversarial network is to match generated distribution with the original data distribution.\n",
        "\n",
        "GANs are an exciting and rapidly changing field, delivering generative models ability to generate realistic examples across a range of problem domains, most notably in image-to-image translation tasks such as translating photos of summer to winter or day to night,coloring images and in generating fake photos that even human cannot categorized as fake image.\n",
        "This <a href = 'https://thispersondoesnotexist.com/'> site </a> uses GAN to generate fake human faces which are similar to real human"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EJyKEP3SKq"
      },
      "source": [
        "### Components of GANs\n",
        "<b> Generator </b>: Generator are neural network that learns to generate data which resemble with the input distribution. The generator model take fixed dimension random vector from Gaussian distribution as input and generate the sample out of it which resemble with input.\n",
        "\n",
        "<b> Discriminator </b>: Discriminator are simple neural network that distinguish fake and real data.The discriminator model takes an example from the domain as input (real or generated) and predicts a binary class label of real or fake.\n",
        "\n",
        "Generative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary. The generator network directly produces samples. Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples drawn from the generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjSgbIaD3SKr"
      },
      "source": [
        "# Working of GANs\n",
        "<img src = 'https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/11000153/g1.jpg'>\n",
        "\n",
        "First of all we take batch of random vector from the Gaussian distribution and generate fake image out of it using generator. Since generator isn't trained so generated image donot resemble with the real input distribution. We take batches of image from the input distribution along with generated fake images and fed it to discriminator so that it learns to distinguish between real and fake images.\n",
        "Now, after training discriminator, we take the batch of images that generator generated and fed them through discriminator again (here we donot fed real images), discriminator will provide an output probabilities, these values are then compared with the probability that the generator should generated (ie 1), error is calculated and backpropagated through the generator and the weight are updated.\n",
        "This above process is repeated until generated images resemble with the input distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsbmydOC3SKr"
      },
      "source": [
        "## Application of GANs\n",
        "\n",
        "## 1. Generating fake faces\n",
        "<img src = \"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Examples-of-Photorealistic-GAN-Generated-Faces.png\">\n",
        "\n",
        "## 2. Generate Examples for Image Datasets\n",
        "\n",
        "## 3. Face Aging\n",
        "<img src = 'https://www.baycare.net/media/5076/botox-aging-face-plastic-surgery.jpg' height = '600px' width = '500px'>\n",
        "\n",
        "\n",
        "\n",
        "## 4. Super Resolution\n",
        "<img src = 'https://miro.medium.com/max/700/1*E-JmUwv7zbwjzFm1hJLxPA.png'>\n",
        "\n",
        "\n",
        "## 5. Image-to-Image Translation\n",
        "<img src = 'https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Example-of-Photographs-of-Daytime-Cityscapes-to-Nighttime-with-pix2pix.png'>\n",
        "\n",
        "## 6.  Photos to Emojis\n",
        "<img src = 'https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Example-of-Celebrity-Photographs-and-GAN-Generated-Emojis.png'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 7.Text to image Translation\n",
        "\n",
        "<img src = 'https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Example-of-Textual-Descriptions-and-GAN-Generated-Photographs-of-Birds.png'>\n",
        "\n",
        "\n",
        "## 8. Generate Cartoon characters\n",
        "<img src = 'https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Example-of-GAN-Generated-Anime-Character-Faces.png'>\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dihqKYHH3SKr"
      },
      "source": [
        "## Objective: To generate fake faces of human"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsCAscx33SKr"
      },
      "source": [
        "## Import necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MwK4CXEq3SKr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from keras.preprocessing.image import img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmvbzy023SKs"
      },
      "source": [
        "## Load data\n",
        "Here I have used face-mask-lite-dataset, out of available 10000 images i have only use 9090 image. I have read image using opencv since opencv reads image in bgr format i have converted it back to rgb format using cvtColor function. These images are resize into 128 by 128 using resize function and are finally converted to array and are appended in empty array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evg0gl2J3SKs",
        "outputId": "d011a2be-66cd-421b-fe4d-67aaa4269266"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 9090/10000 [10:14<01:01, 14.79it/s]\n"
          ]
        }
      ],
      "source": [
        "# to get the files in proper order\n",
        "def sorted_alphanumeric(data):\n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n",
        "    return sorted(data,key = alphanum_key)\n",
        "# defining the size of the image\n",
        "SIZE = 128\n",
        "_img = []\n",
        "path = '../input/face-mask-lite-dataset/without_mask'\n",
        "files = os.listdir(path)\n",
        "files = sorted_alphanumeric(files)\n",
        "for i in tqdm(files):\n",
        "        if i == 'seed9090.png':\n",
        "            break\n",
        "        else:\n",
        "            img = cv2.imread(path + '/'+i,1)\n",
        "            # open cv reads images in BGR format so we have to convert it to RGB\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            #resizing image\n",
        "            img = cv2.resize(img, (SIZE, SIZE))\n",
        "            img = (img - 127.5) / 127.5\n",
        "            imh = img.astype(float)\n",
        "            _img.append(img_to_array(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qHodEZS3SKt"
      },
      "source": [
        "## Visailze our images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUa2BFkS3SKt"
      },
      "outputs": [],
      "source": [
        "def plot_images(sqr = 5):\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.title(\"Real Images\",fontsize = 35)\n",
        "    for i in range(sqr * sqr):\n",
        "        plt.subplot(sqr,sqr,i+1)\n",
        "        plt.imshow(_img[i]*0.5 + 0.5 )\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "# to plot images\n",
        "plot_images(6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIm4JCeC3SKt"
      },
      "source": [
        "Here, i have defined batch size so that these batches of images can be fed directly to the discriminator network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH5iBWQi3SKt"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataset=tf.data.Dataset.from_tensor_slices(np.array(_img)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGzALhat3SKt"
      },
      "source": [
        "# Generator\n",
        "Here, I have defined generator network. It take random vector from normal distribution as input. This random vector is passed through dense layer and is reshaped and is finally fed through Convolution layers. Here, convolution layers does downsampling of our latent vector, after series of convolution batch normalization and leakyrelu layer our downsampled latent vector is upsampled using Conv2DTranspose.\n",
        "\n",
        "The final output layer of Generator generate 128 by 128 by 3 image. The final layer of generator uses hyperbolic tangent as activation to squash the value in between -1 and 1. Generator model looks like simple autoencoder model, where input data is downsampled first and is finally upsampled ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBXcPPg33SKu"
      },
      "outputs": [],
      "source": [
        "latent_dim = 100\n",
        "def Generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(128*128*3, use_bias=False, input_shape=(latent_dim,)))\n",
        "    model.add(layers.Reshape((128,128,3)))\n",
        "    # downsampling\n",
        "    model.add(tf.keras.layers.Conv2D(128,4, strides=1, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Conv2D(256,4, strides=1, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    #upsampling\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(256, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(256, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(3,4,strides = 1, padding = 'same',activation = 'tanh'))\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7lC9TDp3SKu"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ8xL5lR3SKu"
      },
      "source": [
        "# Discriminator\n",
        "Here, discriminator model take 128 by 128 by 3 image that can be real or generated. This input image is downsampled using Convolution layer and is finally flattened and is fed to single neuron so that it can distinguish real and fake image. Since, final layer uses sigmoid function as activation, it output value in between 0 and 1. Here value greater than 0.5 refers to real and less than 0.5 refers to fake image. The output of discriminator is used in training of generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4fpeLzH3SKu"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Input((SIZE, SIZE, 3)))\n",
        "    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(1,activation = 'sigmoid'))\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvyEIIK-3SKu"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator()\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3VX3gT23SKu"
      },
      "source": [
        "### Let's plot image generated by generator before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgUTmi693SKu"
      },
      "outputs": [],
      "source": [
        "noise = np.random.normal(-1,1,(1,100))\n",
        "img = generator(noise)\n",
        "plt.imshow(img[0,:,:,0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2PijA003SKv"
      },
      "source": [
        "### Defining loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFQ3SISF3SKv"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(\n",
        "        lr=.0001,\n",
        "        clipvalue=1.0,\n",
        "        decay=1e-8\n",
        "    )\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-6EhDR93SKv"
      },
      "outputs": [],
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output),fake_output)\n",
        "def discriminator_loss(fake_output, real_output):\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output)\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output),real_output)\n",
        "    return fake_loss + real_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo9iehpW3SKv"
      },
      "source": [
        "### Defining training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARbvqMfJ3SKv"
      },
      "outputs": [],
      "source": [
        "def train_steps(images):\n",
        "    noise = np.random.normal(0,1,(batch_size,latent_dim))\n",
        "    with tf.GradientTape() as gen_tape , tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise)\n",
        "        fake_output = discriminator(generated_images)\n",
        "        real_output = discriminator(images)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        dis_loss = discriminator_loss(fake_output, real_output)\n",
        "\n",
        "\n",
        "    gradient_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradient_of_discriminator = disc_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradient_of_generator,generator.trainable_variables))\n",
        "    optimizer.apply_gradients(zip(gradient_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    loss = {'gen loss':gen_loss,\n",
        "           'disc loss': dis_loss}\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJVW_Q7Q3SKv"
      },
      "source": [
        "## function to plot generated images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGe7GNb53SKv"
      },
      "outputs": [],
      "source": [
        "def plot_generated_images(square = 5, epochs = 0):\n",
        "\n",
        "\n",
        "  plt.figure(figsize = (10,10))\n",
        "  for i in range(square * square):\n",
        "    if epochs != 0:\n",
        "        if(i == square //2):\n",
        "            plt.title(\"Generated Image at Epoch:{}\\n\".format(epochs), fontsize = 32, color = 'black')\n",
        "    plt.subplot(square, square, i+1)\n",
        "    noise = np.random.normal(0,1,(1,latent_dim))\n",
        "    img = generator(noise)\n",
        "    plt.imshow(np.clip((img[0,...]+1)/2, 0, 1))\n",
        "\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1THLSF5s3SKw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def train(epochs,dataset):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        print(\"\\nEpoch : {}\".format(epoch + 1))\n",
        "        for images in dataset:\n",
        "            loss = train_steps(images)\n",
        "        print(\" Time:{}\".format(np.round(time.time() - start),2))\n",
        "        print(\"Generator Loss: {} Discriminator Loss: {}\".format(loss['gen loss'],loss['disc loss']))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfpvJlEL3SKw"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxaBlZNA3SKw",
        "outputId": "8c58f0c7-1b88-41c8-bb9a-863f939711cd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-56b6077cea10>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# i had train model previously for more than 10 epochs so generated images are quiet good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "train(5,dataset)\n",
        "# i had train model previously for more than 10 epochs so generated images are quiet good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ08180A3SKw"
      },
      "source": [
        "# Some Generated Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKynJR-Z3SKw"
      },
      "outputs": [],
      "source": [
        "plot_generated_images(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "simwrPa03SKw"
      },
      "outputs": [],
      "source": [
        "plot_generated_images(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_bcYxKP3SKw"
      },
      "outputs": [],
      "source": [
        "plot_generated_images(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DLCwtex3SKx"
      },
      "outputs": [],
      "source": [
        "plot_generated_images(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1I38_ul3SKx"
      },
      "outputs": [],
      "source": [
        "generator.save('generator.h5')\n",
        "discriminator.save(\"discriminator.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}